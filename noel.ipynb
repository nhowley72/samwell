{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 classes: ['backhand_drive', 'forehand_lift', 'backhand_net_shot', 'forehand_net_shot', 'forehand_drive', 'forehand_clear']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 836 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/noel_personal/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 199\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[16], line 164\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForVideoClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    158\u001b[0m     MODEL_NAME,\n\u001b[1;32m    159\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mnum_labels,\n\u001b[1;32m    160\u001b[0m     ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Needed when changing the number of classes\u001b[39;00m\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Define trainer\u001b[39;00m\n\u001b[1;32m    177\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    178\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    179\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, tp_size, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:1808\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1806\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[0;32m-> 1808\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:2344\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 2344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/generic.py:62\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     60\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/training_args.py:2214\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2215\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2216\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2217\u001b[0m         )\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[1;32m   2219\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoImageProcessor, \n",
    "    AutoModelForVideoClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define paths and parameters\n",
    "DATASET_PATH = \"badminton_dataset\"\n",
    "MODEL_NAME = \"facebook/timesformer-base-finetuned-k400\"\n",
    "OUTPUT_DIR = \"finetuned_timesformer\"\n",
    "NUM_FRAMES = 8  # Number of frames to sample from each video\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Get class labels from directory names\n",
    "class_labels = [d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d)) and not d.startswith('.')]\n",
    "label_to_id = {label: i for i, label in enumerate(class_labels)}\n",
    "id_to_label = {i: label for i, label in enumerate(class_labels)}\n",
    "num_labels = len(class_labels)\n",
    "\n",
    "print(f\"Found {num_labels} classes: {class_labels}\")\n",
    "\n",
    "# Initialize the processor\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class BadmintonVideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, processor):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load video using OpenCV\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            \n",
    "            # Get video properties\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            \n",
    "            if frame_count <= 0:\n",
    "                raise ValueError(f\"Video has no frames: {video_path}\")\n",
    "                \n",
    "            # Sample frames uniformly\n",
    "            if frame_count >= NUM_FRAMES:\n",
    "                indices = np.linspace(0, frame_count - 1, NUM_FRAMES, dtype=int)\n",
    "            else:\n",
    "                # If video is shorter, loop frames\n",
    "                indices = np.arange(frame_count).repeat(NUM_FRAMES // frame_count + 1)[:NUM_FRAMES]\n",
    "            \n",
    "            # Read selected frames\n",
    "            for frame_idx in indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                # Convert BGR to RGB\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frames.append(frame)\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            # Check if we have enough frames\n",
    "            if len(frames) < NUM_FRAMES:\n",
    "                # If we don't have enough frames, duplicate the last frame\n",
    "                last_frame = frames[-1] if frames else np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "                while len(frames) < NUM_FRAMES:\n",
    "                    frames.append(last_frame)\n",
    "            \n",
    "            # Process frames with the TimeSformer processor\n",
    "            inputs = self.processor(\n",
    "                frames,  # List of frames\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            \n",
    "            # Add label\n",
    "            inputs[\"labels\"] = torch.tensor(label)\n",
    "            \n",
    "            return inputs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {e}\")\n",
    "            # Return a placeholder with the correct structure\n",
    "            # This is a fallback to avoid breaking the training loop\n",
    "            dummy_inputs = {\n",
    "                \"pixel_values\": torch.zeros((1, NUM_FRAMES, 3, 224, 224)),\n",
    "                \"labels\": torch.tensor(label)\n",
    "            }\n",
    "            return dummy_inputs\n",
    "\n",
    "def collect_videos():\n",
    "    video_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for class_name in class_labels:\n",
    "        class_dir = os.path.join(DATASET_PATH, class_name)\n",
    "        class_id = label_to_id[class_name]\n",
    "        \n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "                video_path = os.path.join(class_dir, filename)\n",
    "                video_paths.append(video_path)\n",
    "                labels.append(class_id)\n",
    "    \n",
    "    return video_paths, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Filter out None values (failed video processing)\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    pixel_values = torch.cat([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Collect video paths and labels\n",
    "    video_paths, labels = collect_videos()\n",
    "    print(f\"Found {len(video_paths)} videos\")\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_videos, val_videos, train_labels, val_labels = train_test_split(\n",
    "        video_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BadmintonVideoDataset(train_videos, train_labels, processor)\n",
    "    val_dataset = BadmintonVideoDataset(val_videos, val_labels, processor)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForVideoClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True  # Needed when changing the number of classes\n",
    "    )\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    # Define trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=collate_fn,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    processor.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")\n",
    "    \n",
    "    print(f\"Model saved to {os.path.join(OUTPUT_DIR, 'final_model')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 1)) (2.6.0)\n",
      "Requirement already satisfied: transformers>=4.18.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 2)) (4.50.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
      "Collecting opencv-python>=4.5.0\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 6)) (3.5.0)\n",
      "Collecting av~=10.0.0\n",
      "  Downloading av-10.0.0-cp39-cp39-macosx_11_0_arm64.whl (19.6 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19.6 MB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.10.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: networkx in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: filelock in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: fsspec in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from torch>=1.10.0->-r requirements.txt (line 1)) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.10.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (0.30.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from transformers>=4.18.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->-r requirements.txt (line 6)) (3.11.16)\n",
      "Requirement already satisfied: xxhash in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->-r requirements.txt (line 6)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->-r requirements.txt (line 6)) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->-r requirements.txt (line 6)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->-r requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from datasets>=2.0.0->-r requirements.txt (line 6)) (19.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (6.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (2.6.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (1.18.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 6)) (25.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.18.0->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.18.0->-r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.18.0->-r requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from requests->transformers>=4.18.0->-r requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.10.0->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/noel_personal/Library/Python/3.9/lib/python/site-packages (from pandas->datasets>=2.0.0->-r requirements.txt (line 6)) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.0.0->-r requirements.txt (line 6)) (1.15.0)\n",
      "Installing collected packages: opencv-python, av\n",
      "  Attempting uninstall: av\n",
      "    Found existing installation: av 14.3.0\n",
      "    Uninstalling av-14.3.0:\n",
      "      Successfully uninstalled av-14.3.0\n",
      "Successfully installed av-10.0.0 opencv-python-4.11.0.86\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
